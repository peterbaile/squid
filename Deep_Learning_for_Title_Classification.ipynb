{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep Learning for Title Classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterbaile/squid/blob/master/Deep_Learning_for_Title_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R0hQULBhOOv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.utils import resample\n",
        "import sklearn\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "tqdm.pandas()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3yE3GJY7SLq"
      },
      "source": [
        "Last Updated 2 Dec\n",
        "\n",
        "**Housekeeping**\n",
        "\n",
        "1. Download tensorflow_gpu (to enable much quicker training)\n",
        "2. Download eli5\n",
        "3. Download scikit-learn==0.21.3 (to enable text highlighting visualization of the eli5 explanations) https://github.com/TeamHG-Memex/eli5/issues/361\n",
        "\n",
        "**Workflow**\n",
        "\n",
        "1. Preprocessing raw text data\n",
        "2. Loading existing word embeddings to create embedding matrix\n",
        "3. Train RNN model (GRU) to classify documents into quintiles\n",
        "4. Evaluating Model (Confusion Matrix)\n",
        "5. Explainable Model Insights (contribution of each word to prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wPjyw5RiRue",
        "outputId": "72f08f4c-2a48-40c4-a18d-b031d4bf0f87"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U11V9cSUhW0f",
        "outputId": "8636677a-064a-4398-9530-c5700c941a21"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3OEo7bkhW4u"
      },
      "source": [
        "#train = pd.read_csv('drive/MyDrive/CIS520 Project/train.csv')\n",
        "# Use when I'm using Wharton account\n",
        "train = pd.read_csv('drive/MyDrive/CIS520 Project/data set/train.csv')\n",
        "test = pd.read_csv('drive/MyDrive/CIS520 Project/data set/test.csv')\n",
        "test_final = pd.read_csv('drive/MyDrive/CIS520 Project/data set/test_df_upsampled.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_CRzJkHg8nq"
      },
      "source": [
        "# Get top 25% vs bottom 75%\r\n",
        "train['top25pct'] = (train['percentile'] >= 0.75).astype(int)\r\n",
        "test['top25pct'] = (test['percentile'] >= 0.75).astype(int)\r\n",
        "\r\n",
        "train['published_date'] = train['published_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\r\n",
        "test['published_date'] = test['published_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOoOBFaJg_7t"
      },
      "source": [
        "def upsample_minority(df):\r\n",
        "\r\n",
        "  # Upsample minority class in both the training and test data\r\n",
        "  df_majority = df.loc[df['top25pct'] == 0, :]\r\n",
        "  df_minority = df.loc[df['top25pct'] == 1, :]\r\n",
        "  df_minority_upsampled = resample(df_minority, replace = True, n_samples = len(df_majority), random_state = 42)\r\n",
        "\r\n",
        "  # Combine together to get the upsampled training data\r\n",
        "  df = pd.concat([df_majority, df_minority_upsampled])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pqY9Pv2lzp6"
      },
      "source": [
        "def downsample_majority(df):\r\n",
        "\r\n",
        "  # Upsample minority class in both the training and test data\r\n",
        "  df_majority = df.loc[df['top25pct'] == 0, :]\r\n",
        "  df_minority = df.loc[df['top25pct'] == 1, :]\r\n",
        "  df_majority_downsampled = resample(df_majority, replace = True, n_samples = len(df_minority), random_state = 42)\r\n",
        "\r\n",
        "  # Combine together to get the upsampled training data\r\n",
        "  df = pd.concat([df_minority, df_majority_downsampled])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tik6vf_ChG8A"
      },
      "source": [
        "# Upsample the minority class\r\n",
        "train = upsample_minority(train)\r\n",
        "test = upsample_minority(test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73xtwczJhW-y"
      },
      "source": [
        "def preprocessing(content_list):\n",
        "    \n",
        "    processed_list = []\n",
        "    \n",
        "    for line in tqdm(content_list):\n",
        "        tokens = word_tokenize(line)\n",
        "        # Convert to lower case\n",
        "        tokens = [w.lower() for w in tokens]\n",
        "        # Remove punctuation\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        # Remove remaining tokens that are not alphabetic\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        # Filter out stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        \n",
        "        processed_list.append(words)\n",
        "        \n",
        "    return processed_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYR79wyJiC4k",
        "outputId": "af0ac388-e841-4764-94ea-c9f2a401682a"
      },
      "source": [
        "# Preprocessing the words\n",
        "train['processed_content'] = preprocessing(train['content'])\n",
        "train['processed_title'] = preprocessing(train['title'])\n",
        "\n",
        "test['processed_content'] = preprocessing(test['content'])\n",
        "test['processed_title'] = preprocessing(test['title'])\n",
        "\n",
        "test_final['processed_title'] = preprocessing(test_final['title'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25084/25084 [02:00<00:00, 208.43it/s]\n",
            "100%|██████████| 25084/25084 [00:07<00:00, 3476.80it/s]\n",
            "100%|██████████| 6240/6240 [00:30<00:00, 207.82it/s]\n",
            "100%|██████████| 6240/6240 [00:01<00:00, 3409.51it/s]\n",
            "100%|██████████| 6240/6240 [00:01<00:00, 3526.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rF_jyZx2A5K"
      },
      "source": [
        "# Shuffle test again, and reset index (very important!!!)\r\n",
        "test = test.sample(frac = 1)\r\n",
        "test = test.reset_index(drop = True)\r\n",
        "train = train.reset_index(drop = True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xo1RVhRiIq-"
      },
      "source": [
        "**Fitting Word Embeddings**\n",
        "\n",
        "Word embeddings were trained using the notebook Word Embeddings.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti0OOP2qiC6r"
      },
      "source": [
        "# Extract the embeddings from the stored file\n",
        "# Embedding is size 111k (# words) x 100 (dimensions)\n",
        "import os \n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('drive/MyDrive/CIS520 Project', 'word2vec_train2.txt'), encoding = 'utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKtEWZGPiC8z"
      },
      "source": [
        "def vectorize_text(content):\n",
        "\n",
        "  # Vectorize the text samples (now TITLES) into 2D integer tensor - max length 16 words\n",
        "  tokenizer_obj = Tokenizer()\n",
        "  # Fit the tokenizer on the text\n",
        "  tokenizer_obj.fit_on_texts(content)\n",
        "  # Generate the sequence of tokens\n",
        "  sequences = tokenizer_obj.texts_to_sequences(content)\n",
        "\n",
        "  # Get the max length of each article - 5587\n",
        "  max_length = max([len(s) for s in content])\n",
        "  # Pad the sequences\n",
        "  vectorized_text = pad_sequences(sequences, maxlen = max_length)\n",
        "\n",
        "  return vectorized_text, tokenizer_obj, max_length"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il2tcU_IiC_d"
      },
      "source": [
        "def get_embedding_matrix(tokenizer_obj, EMBEDDING_DIM = 100):\n",
        " \n",
        "  word_index = tokenizer_obj.word_index\n",
        "\n",
        "  num_words = len(word_index) + 1\n",
        "  words_not_found = []\n",
        "  # Create the emedding matrix - map embeddings from word2vec model for each word and create matrix of word vectors\n",
        "  embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "  for word, i in word_index.items():\n",
        "      if i > num_words: # Least common words (don't care)\n",
        "          continue\n",
        "          \n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      \n",
        "      if (embedding_vector is not None):\n",
        "          # Assign the ith elmenet of the embedding matrix to the embedding of that word\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "      else:\n",
        "          words_not_found.append(word)\n",
        "          \n",
        "  print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XHrGM4Zsmbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990f3859-d290-4441-9dbb-8b4ad79ab4ee"
      },
      "source": [
        "# Vectorize the text (return document x length matrix)\r\n",
        "train_vectorized, tokenizer, max_length = vectorize_text(train['processed_title'])\r\n",
        "\r\n",
        "test_vectorized = tokenizer.texts_to_sequences(test['processed_title'])\r\n",
        "test_vectorized = pad_sequences(test_vectorized, maxlen = max_length)\r\n",
        "\r\n",
        "test_final_vectorized = tokenizer.texts_to_sequences(test_final['processed_title'])\r\n",
        "test_final_vectorized = pad_sequences(test_final_vectorized, maxlen = max_length)\r\n",
        "\r\n",
        "# Get the embedding matrix of the words\r\n",
        "embedding_matrix = get_embedding_matrix(tokenizer)\r\n",
        "num_words = embedding_matrix.shape[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of null word embeddings: 398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaUfltKdifxF"
      },
      "source": [
        "**Training Deep Learning Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGElUT8bie6L"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Embedding, LSTM, GRU, SpatialDropout1D, Bidirectional, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import SGD, Adam\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgw1NWXKr7Li"
      },
      "source": [
        "# Original RNN Model\r\n",
        "def RNN_Model():\r\n",
        "    \r\n",
        "    text_sequence = Input(shape = (max_length,), name = 'text_sequence_input')\r\n",
        "    rnn_layer = Embedding(num_words, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False, name = 'embedding')(text_sequence)\r\n",
        "    rnn_layer = GRU(units = 32, dropout = 0.1)(rnn_layer)\r\n",
        "    rnn_layer = Dense(32, activation = 'relu')(rnn_layer)\r\n",
        "    output = Dense(1, name = 'output')(rnn_layer)\r\n",
        "    model = Model(inputs = text_sequence, outputs = output)\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjlQKXhRzq_S"
      },
      "source": [
        "**Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1s3iIuWifAl",
        "outputId": "8e8e3e16-f8f1-45b9-bda0-ac98b1af7a4f"
      },
      "source": [
        "# Split into train and validation set\n",
        "VALIDATION_SPLIT = 0.2\n",
        "dl_train, dl_val = train_test_split(train, test_size = VALIDATION_SPLIT, random_state = 42, stratify = train['top25pct'])\n",
        "\n",
        "train_indices = dl_train.index.tolist()\n",
        "val_indices = dl_val.index.tolist()\n",
        "\n",
        "# Get the training and validation data\n",
        "X_train = train_vectorized[train_indices]\n",
        "X_val = train_vectorized[val_indices]\n",
        "X_test = test_vectorized\n",
        "\n",
        "y_train = dl_train['top25pct'].to_numpy()\n",
        "y_val = dl_val['top25pct'].to_numpy()\n",
        "y_test = test['top25pct'].to_numpy()\n",
        "\n",
        "print('Shape of X_train: ', X_train.shape)\n",
        "print('Shape of y_train: ', y_train.shape)\n",
        "print('Shape of X_val: ', X_val.shape)\n",
        "print('Shape of y_val: ', y_val.shape)\n",
        "print('Shape of X_test: ', X_test.shape)\n",
        "print('Shape of y_test: ', y_test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (20067, 16)\n",
            "Shape of y_train:  (20067,)\n",
            "Shape of X_val:  (5017, 16)\n",
            "Shape of y_val:  (5017,)\n",
            "Shape of X_test:  (6240, 16)\n",
            "Shape of y_test:  (6240,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3IUjsQU7vDO"
      },
      "source": [
        "HP_DROPOUT = [0.5]\r\n",
        "HP_L2 = [0.0001, 0.001]\r\n",
        "HP_BATCH_NORM = [True, False]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKPdrwyfzqj6"
      },
      "source": [
        "# GRU model - for hyperparameter tuning\r\n",
        "def RNN_Model(hp):\r\n",
        "    \r\n",
        "    text_sequence = Input(shape = (max_length,), name = 'text_sequence_input')\r\n",
        "    rnn_layer = Embedding(num_words, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False, name = 'embedding')(text_sequence)\r\n",
        "    rnn_layer = Dropout(hp['dropout'])(rnn_layer)\r\n",
        "    rnn_layer = GRU(units = 32, dropout = hp['dropout'],  recurrent_regularizer = l2(hp['l2']))(rnn_layer)\r\n",
        "    if hp['batch_norm'] == True:\r\n",
        "      rnn_layer = BatchNormalization()(rnn_layer)\r\n",
        "    rnn_layer = Dense(32, activation = 'relu', name = 'dense', kernel_regularizer = l2(hp['l2']))(rnn_layer)\r\n",
        "    output = Dense(1, name = 'output')(rnn_layer) # Change to 1 if it's just classification\r\n",
        "    \r\n",
        "    model = Model(inputs = text_sequence, outputs = output)\r\n",
        "    return model"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxGmps342tXh",
        "outputId": "aeaa4b4b-5663-4ec9-ae4c-67622192138f"
      },
      "source": [
        "# Running the hyperparameter tuning\r\n",
        "\r\n",
        "tuning_results = []\r\n",
        "\r\n",
        "for hp_dropout in HP_DROPOUT:\r\n",
        "  for hp_l2 in HP_L2:\r\n",
        "    for hp_batch_norm in HP_BATCH_NORM:\r\n",
        "\r\n",
        "          hp = {'dropout': hp_dropout, 'l2': hp_l2, 'batch_norm': hp_batch_norm}\r\n",
        "          model = RNN_Model(hp)\r\n",
        "\r\n",
        "          early_stopping = EarlyStopping(monitor = 'val_accuracy', patience = 5, restore_best_weights=True)\r\n",
        "          model.compile(loss = keras.losses.BinaryCrossentropy(from_logits = True), optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])\r\n",
        "\r\n",
        "          history = model.fit(X_train, y_train, batch_size = 32, epochs = 20, validation_data = (X_val, y_val), verbose = 0,\r\n",
        "                  callbacks = [early_stopping, model_checkpoint])\r\n",
        "          \r\n",
        "          result = {key: max(value) for key, value in history.history.items()}\r\n",
        "          test_result = model.evaluate(X_test, y_test)\r\n",
        "          result.update({'test_loss': test_result[0], 'test_accuracy': test_result[1]})\r\n",
        "\r\n",
        "          # Unfreeze embedding layer\r\n",
        "          model.layers[1].trainable = True\r\n",
        "          model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.0005), metrics = ['accuracy'])\r\n",
        "          history = model.fit(X_train, y_train, batch_size = 32, epochs = 20, validation_data = (X_val, y_val), verbose = 0,\r\n",
        "                  callbacks = [early_stopping, model_checkpoint])\r\n",
        "\r\n",
        "\r\n",
        "          result_end = {key: max(value) for key, value in history.history.items()}\r\n",
        "          test_result_end = model.evaluate(X_test, y_test)\r\n",
        "          result_end.update({'test_loss': test_result_end[0], 'test_accuracy': test_result_end[1]})\r\n",
        "\r\n",
        "          result_final = {'frozen': result, 'unfrozen': result_end}\r\n",
        "          tuning_results.append(result_final)\r\n",
        "\r\n",
        "          print((hp_dropout, hp_l2, hp_batch_norm))\r\n",
        "          print(result_final)\r\n",
        "          print(datetime.now())"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195/195 [==============================] - 0s 3ms/step - loss: 0.5899 - accuracy: 0.6646\n",
            "195/195 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6995\n",
            "(0.5, 0.0001, True)\n",
            "{'frozen': {'loss': 0.6885411739349365, 'accuracy': 0.6532117128372192, 'val_loss': 0.6325933933258057, 'val_accuracy': 0.7067968845367432, 'test_loss': 0.5898725390434265, 'test_accuracy': 0.6645833253860474}, 'unfrozen': {'loss': 1.6152663230895996, 'accuracy': 0.7317984700202942, 'val_loss': 0.9072498083114624, 'val_accuracy': 0.7546342611312866, 'test_loss': 0.6317951083183289, 'test_accuracy': 0.6995192170143127}}\n",
            "2020-12-10 14:48:05.784810\n",
            "195/195 [==============================] - 0s 2ms/step - loss: 0.6089 - accuracy: 0.6649\n",
            "195/195 [==============================] - 0s 2ms/step - loss: 0.7844 - accuracy: 0.6909\n",
            "(0.5, 0.0001, False)\n",
            "{'frozen': {'loss': 0.6741644740104675, 'accuracy': 0.6433448195457458, 'val_loss': 0.6233368515968323, 'val_accuracy': 0.6950368881225586, 'test_loss': 0.6089453101158142, 'test_accuracy': 0.6649038195610046}, 'unfrozen': {'loss': 1.174093246459961, 'accuracy': 0.7914984822273254, 'val_loss': 0.7828571796417236, 'val_accuracy': 0.7907115817070007, 'test_loss': 0.7844148874282837, 'test_accuracy': 0.6908653974533081}}\n",
            "2020-12-10 14:51:23.177793\n",
            "195/195 [==============================] - 0s 3ms/step - loss: 0.5906 - accuracy: 0.6681\n",
            "195/195 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.7099\n",
            "(0.5, 0.001, True)\n",
            "{'frozen': {'loss': 0.7200682163238525, 'accuracy': 0.6462351083755493, 'val_loss': 0.6557743549346924, 'val_accuracy': 0.6942395567893982, 'test_loss': 0.59059077501297, 'test_accuracy': 0.6681089997291565}, 'unfrozen': {'loss': 1.7896382808685303, 'accuracy': 0.7674291133880615, 'val_loss': 1.0498759746551514, 'val_accuracy': 0.7749651074409485, 'test_loss': 0.6672863960266113, 'test_accuracy': 0.7099359035491943}}\n",
            "2020-12-10 14:55:23.406480\n",
            "195/195 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6636\n",
            "195/195 [==============================] - 0s 2ms/step - loss: 0.7769 - accuracy: 0.6915\n",
            "(0.5, 0.001, False)\n",
            "{'frozen': {'loss': 0.7057244777679443, 'accuracy': 0.6430457830429077, 'val_loss': 0.6492828130722046, 'val_accuracy': 0.6974287629127502, 'test_loss': 0.6084185838699341, 'test_accuracy': 0.6636217832565308}, 'unfrozen': {'loss': 1.4123520851135254, 'accuracy': 0.794887125492096, 'val_loss': 0.7872603535652161, 'val_accuracy': 0.7893162965774536, 'test_loss': 0.7768874168395996, 'test_accuracy': 0.6915063858032227}}\n",
            "2020-12-10 14:58:50.805400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNA1aZXXiadh"
      },
      "source": [
        "# Evaluating Hyperparameter tuning results\r\n",
        "\r\n",
        "params = []\r\n",
        "\r\n",
        "for hp_dropout in HP_DROPOUT:\r\n",
        "  for hp_l2 in HP_L2:\r\n",
        "    for hp_batch_norm in HP_BATCH_NORM:\r\n",
        "      hp = {'dropout': hp_dropout, 'l2': hp_l2, 'batch_norm': hp_batch_norm}\r\n",
        "      params.append(hp)\r\n",
        "\r\n",
        "tuning_results_frozen_df = pd.DataFrame(data = [e['frozen'] for e in tuning_results])\r\n",
        "tuning_results_unfrozen_df = pd.DataFrame(data = [e['unfrozen'] for e in tuning_results])\r\n",
        "params_df = pd.DataFrame(params)\r\n",
        "tuning_results_df = pd.concat([tuning_results_frozen_df, tuning_results_unfrozen_df, params_df], axis = 1)\r\n",
        "\r\n",
        "tuning_results_df.columns = ['loss_f', 'accuracy_f', 'val_loss_f', 'val_accuracy_f', 'test_loss_f', 'test_accuracy_f',\r\n",
        "                             'loss_u', 'accuracy_u', 'val_loss_u', 'val_accuracy_u', 'test_loss_u', 'test_accuracy_u',\r\n",
        "                             'dropout', 'l2', 'batch_norm']\r\n",
        "\r\n",
        "tuning_results_df.to_csv('drive/MyDrive/CIS520 Project/deep_learning_tuning_results.csv')"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "3AOxof66-qah",
        "outputId": "cc64f96c-ac2e-4718-8331-839390c97331"
      },
      "source": [
        "tuning_results_df.sort_values('test_accuracy_u', ascending = False)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_f</th>\n",
              "      <th>accuracy_f</th>\n",
              "      <th>val_loss_f</th>\n",
              "      <th>val_accuracy_f</th>\n",
              "      <th>test_loss_f</th>\n",
              "      <th>test_accuracy_f</th>\n",
              "      <th>loss_u</th>\n",
              "      <th>accuracy_u</th>\n",
              "      <th>val_loss_u</th>\n",
              "      <th>val_accuracy_u</th>\n",
              "      <th>test_loss_u</th>\n",
              "      <th>test_accuracy_u</th>\n",
              "      <th>dropout</th>\n",
              "      <th>l2</th>\n",
              "      <th>batch_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.720068</td>\n",
              "      <td>0.646235</td>\n",
              "      <td>0.655774</td>\n",
              "      <td>0.694240</td>\n",
              "      <td>0.590591</td>\n",
              "      <td>0.668109</td>\n",
              "      <td>1.789638</td>\n",
              "      <td>0.767429</td>\n",
              "      <td>1.049876</td>\n",
              "      <td>0.774965</td>\n",
              "      <td>0.667286</td>\n",
              "      <td>0.709936</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.688541</td>\n",
              "      <td>0.653212</td>\n",
              "      <td>0.632593</td>\n",
              "      <td>0.706797</td>\n",
              "      <td>0.589873</td>\n",
              "      <td>0.664583</td>\n",
              "      <td>1.615266</td>\n",
              "      <td>0.731798</td>\n",
              "      <td>0.907250</td>\n",
              "      <td>0.754634</td>\n",
              "      <td>0.631795</td>\n",
              "      <td>0.699519</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.714453</td>\n",
              "      <td>0.670853</td>\n",
              "      <td>0.643762</td>\n",
              "      <td>0.706797</td>\n",
              "      <td>0.595248</td>\n",
              "      <td>0.676603</td>\n",
              "      <td>1.532631</td>\n",
              "      <td>0.804904</td>\n",
              "      <td>0.949449</td>\n",
              "      <td>0.800080</td>\n",
              "      <td>0.846200</td>\n",
              "      <td>0.694231</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.705724</td>\n",
              "      <td>0.643046</td>\n",
              "      <td>0.649283</td>\n",
              "      <td>0.697429</td>\n",
              "      <td>0.608419</td>\n",
              "      <td>0.663622</td>\n",
              "      <td>1.412352</td>\n",
              "      <td>0.794887</td>\n",
              "      <td>0.787260</td>\n",
              "      <td>0.789316</td>\n",
              "      <td>0.776887</td>\n",
              "      <td>0.691506</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.674164</td>\n",
              "      <td>0.643345</td>\n",
              "      <td>0.623337</td>\n",
              "      <td>0.695037</td>\n",
              "      <td>0.608945</td>\n",
              "      <td>0.664904</td>\n",
              "      <td>1.174093</td>\n",
              "      <td>0.791498</td>\n",
              "      <td>0.782857</td>\n",
              "      <td>0.790712</td>\n",
              "      <td>0.784415</td>\n",
              "      <td>0.690865</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.658957</td>\n",
              "      <td>0.717995</td>\n",
              "      <td>0.608367</td>\n",
              "      <td>0.742675</td>\n",
              "      <td>0.589851</td>\n",
              "      <td>0.680929</td>\n",
              "      <td>2.142535</td>\n",
              "      <td>0.725121</td>\n",
              "      <td>1.833577</td>\n",
              "      <td>0.752242</td>\n",
              "      <td>2.628977</td>\n",
              "      <td>0.686538</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.675346</td>\n",
              "      <td>0.682264</td>\n",
              "      <td>0.621037</td>\n",
              "      <td>0.722942</td>\n",
              "      <td>0.580429</td>\n",
              "      <td>0.684135</td>\n",
              "      <td>1.929438</td>\n",
              "      <td>0.761798</td>\n",
              "      <td>1.500578</td>\n",
              "      <td>0.777756</td>\n",
              "      <td>0.798151</td>\n",
              "      <td>0.686058</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.647551</td>\n",
              "      <td>0.753725</td>\n",
              "      <td>0.587832</td>\n",
              "      <td>0.759019</td>\n",
              "      <td>0.619663</td>\n",
              "      <td>0.665865</td>\n",
              "      <td>1.991590</td>\n",
              "      <td>0.785818</td>\n",
              "      <td>1.892266</td>\n",
              "      <td>0.777158</td>\n",
              "      <td>3.017423</td>\n",
              "      <td>0.684455</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.697662</td>\n",
              "      <td>0.703045</td>\n",
              "      <td>0.628461</td>\n",
              "      <td>0.729918</td>\n",
              "      <td>0.580442</td>\n",
              "      <td>0.687019</td>\n",
              "      <td>2.010686</td>\n",
              "      <td>0.709224</td>\n",
              "      <td>1.707246</td>\n",
              "      <td>0.726530</td>\n",
              "      <td>2.181866</td>\n",
              "      <td>0.681410</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.687237</td>\n",
              "      <td>0.743360</td>\n",
              "      <td>0.617762</td>\n",
              "      <td>0.756827</td>\n",
              "      <td>0.618128</td>\n",
              "      <td>0.664423</td>\n",
              "      <td>2.035980</td>\n",
              "      <td>0.786316</td>\n",
              "      <td>2.036619</td>\n",
              "      <td>0.779550</td>\n",
              "      <td>2.650440</td>\n",
              "      <td>0.674679</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.659742</td>\n",
              "      <td>0.746001</td>\n",
              "      <td>0.601636</td>\n",
              "      <td>0.755033</td>\n",
              "      <td>0.615039</td>\n",
              "      <td>0.668429</td>\n",
              "      <td>1.924658</td>\n",
              "      <td>0.895699</td>\n",
              "      <td>1.831151</td>\n",
              "      <td>0.827786</td>\n",
              "      <td>1.412203</td>\n",
              "      <td>0.674199</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.635557</td>\n",
              "      <td>0.757164</td>\n",
              "      <td>0.586625</td>\n",
              "      <td>0.762208</td>\n",
              "      <td>0.600115</td>\n",
              "      <td>0.678205</td>\n",
              "      <td>1.722007</td>\n",
              "      <td>0.903972</td>\n",
              "      <td>1.290696</td>\n",
              "      <td>0.846920</td>\n",
              "      <td>1.485427</td>\n",
              "      <td>0.671795</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.679728</td>\n",
              "      <td>0.740170</td>\n",
              "      <td>0.619116</td>\n",
              "      <td>0.756428</td>\n",
              "      <td>0.607319</td>\n",
              "      <td>0.670994</td>\n",
              "      <td>2.076202</td>\n",
              "      <td>0.845866</td>\n",
              "      <td>1.962356</td>\n",
              "      <td>0.818019</td>\n",
              "      <td>1.090282</td>\n",
              "      <td>0.670192</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.646724</td>\n",
              "      <td>0.755071</td>\n",
              "      <td>0.590270</td>\n",
              "      <td>0.768587</td>\n",
              "      <td>0.620087</td>\n",
              "      <td>0.672596</td>\n",
              "      <td>2.152048</td>\n",
              "      <td>0.846365</td>\n",
              "      <td>2.228838</td>\n",
              "      <td>0.816823</td>\n",
              "      <td>1.807081</td>\n",
              "      <td>0.665385</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.827478</td>\n",
              "      <td>0.726217</td>\n",
              "      <td>0.650141</td>\n",
              "      <td>0.733905</td>\n",
              "      <td>0.615450</td>\n",
              "      <td>0.672596</td>\n",
              "      <td>2.162189</td>\n",
              "      <td>0.883590</td>\n",
              "      <td>2.000609</td>\n",
              "      <td>0.834562</td>\n",
              "      <td>1.746702</td>\n",
              "      <td>0.664904</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.631303</td>\n",
              "      <td>0.806648</td>\n",
              "      <td>0.574442</td>\n",
              "      <td>0.805063</td>\n",
              "      <td>0.703643</td>\n",
              "      <td>0.665385</td>\n",
              "      <td>1.846606</td>\n",
              "      <td>0.854537</td>\n",
              "      <td>2.195577</td>\n",
              "      <td>0.820610</td>\n",
              "      <td>3.974023</td>\n",
              "      <td>0.658974</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.761266</td>\n",
              "      <td>0.770519</td>\n",
              "      <td>0.618038</td>\n",
              "      <td>0.761411</td>\n",
              "      <td>0.652031</td>\n",
              "      <td>0.663622</td>\n",
              "      <td>2.046763</td>\n",
              "      <td>0.937908</td>\n",
              "      <td>2.065458</td>\n",
              "      <td>0.859278</td>\n",
              "      <td>2.245854</td>\n",
              "      <td>0.656570</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.617374</td>\n",
              "      <td>0.825036</td>\n",
              "      <td>0.570751</td>\n",
              "      <td>0.814431</td>\n",
              "      <td>0.751564</td>\n",
              "      <td>0.655769</td>\n",
              "      <td>1.775985</td>\n",
              "      <td>0.931131</td>\n",
              "      <td>2.124832</td>\n",
              "      <td>0.869643</td>\n",
              "      <td>3.895015</td>\n",
              "      <td>0.651122</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.817157</td>\n",
              "      <td>0.765236</td>\n",
              "      <td>0.642579</td>\n",
              "      <td>0.773769</td>\n",
              "      <td>0.647118</td>\n",
              "      <td>0.656410</td>\n",
              "      <td>2.056852</td>\n",
              "      <td>0.867295</td>\n",
              "      <td>2.293686</td>\n",
              "      <td>0.821407</td>\n",
              "      <td>1.861733</td>\n",
              "      <td>0.649038</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.672446</td>\n",
              "      <td>0.788409</td>\n",
              "      <td>0.606914</td>\n",
              "      <td>0.787323</td>\n",
              "      <td>0.678935</td>\n",
              "      <td>0.679167</td>\n",
              "      <td>1.934549</td>\n",
              "      <td>0.890317</td>\n",
              "      <td>2.205954</td>\n",
              "      <td>0.843731</td>\n",
              "      <td>3.287028</td>\n",
              "      <td>0.645673</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.793923</td>\n",
              "      <td>0.847062</td>\n",
              "      <td>0.629595</td>\n",
              "      <td>0.794299</td>\n",
              "      <td>0.929567</td>\n",
              "      <td>0.627724</td>\n",
              "      <td>2.081123</td>\n",
              "      <td>0.954851</td>\n",
              "      <td>2.698672</td>\n",
              "      <td>0.862069</td>\n",
              "      <td>4.363837</td>\n",
              "      <td>0.642308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.774960</td>\n",
              "      <td>0.729107</td>\n",
              "      <td>0.625415</td>\n",
              "      <td>0.741878</td>\n",
              "      <td>0.611153</td>\n",
              "      <td>0.656570</td>\n",
              "      <td>1.942109</td>\n",
              "      <td>0.886829</td>\n",
              "      <td>1.571271</td>\n",
              "      <td>0.836356</td>\n",
              "      <td>1.440965</td>\n",
              "      <td>0.641827</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.639962</td>\n",
              "      <td>0.800169</td>\n",
              "      <td>0.586478</td>\n",
              "      <td>0.792107</td>\n",
              "      <td>0.724302</td>\n",
              "      <td>0.645353</td>\n",
              "      <td>2.346301</td>\n",
              "      <td>0.932626</td>\n",
              "      <td>2.813666</td>\n",
              "      <td>0.860076</td>\n",
              "      <td>3.567761</td>\n",
              "      <td>0.639744</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.631846</td>\n",
              "      <td>0.922161</td>\n",
              "      <td>0.581065</td>\n",
              "      <td>0.835958</td>\n",
              "      <td>1.466477</td>\n",
              "      <td>0.643269</td>\n",
              "      <td>1.707211</td>\n",
              "      <td>0.972741</td>\n",
              "      <td>3.508686</td>\n",
              "      <td>0.874626</td>\n",
              "      <td>4.986544</td>\n",
              "      <td>0.635417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.601899</td>\n",
              "      <td>0.937011</td>\n",
              "      <td>0.560864</td>\n",
              "      <td>0.854694</td>\n",
              "      <td>1.669249</td>\n",
              "      <td>0.614904</td>\n",
              "      <td>1.324825</td>\n",
              "      <td>0.948522</td>\n",
              "      <td>2.796253</td>\n",
              "      <td>0.856887</td>\n",
              "      <td>5.282236</td>\n",
              "      <td>0.630769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.735450</td>\n",
              "      <td>0.866746</td>\n",
              "      <td>0.610516</td>\n",
              "      <td>0.797289</td>\n",
              "      <td>0.962477</td>\n",
              "      <td>0.649038</td>\n",
              "      <td>1.537838</td>\n",
              "      <td>0.951811</td>\n",
              "      <td>2.484292</td>\n",
              "      <td>0.855093</td>\n",
              "      <td>4.430211</td>\n",
              "      <td>0.626282</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.644315</td>\n",
              "      <td>0.902626</td>\n",
              "      <td>0.592149</td>\n",
              "      <td>0.830775</td>\n",
              "      <td>1.162312</td>\n",
              "      <td>0.624038</td>\n",
              "      <td>1.709008</td>\n",
              "      <td>0.949868</td>\n",
              "      <td>3.494871</td>\n",
              "      <td>0.855890</td>\n",
              "      <td>4.948723</td>\n",
              "      <td>0.625962</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.597124</td>\n",
              "      <td>0.962376</td>\n",
              "      <td>0.703883</td>\n",
              "      <td>0.852502</td>\n",
              "      <td>2.057860</td>\n",
              "      <td>0.617308</td>\n",
              "      <td>1.221489</td>\n",
              "      <td>0.960682</td>\n",
              "      <td>2.862325</td>\n",
              "      <td>0.851505</td>\n",
              "      <td>5.558369</td>\n",
              "      <td>0.616827</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      loss_f  accuracy_f  val_loss_f  ...  dropout      l2  batch_norm\n",
              "26  0.720068    0.646235    0.655774  ...      0.5  0.0010        True\n",
              "24  0.688541    0.653212    0.632593  ...      0.5  0.0001        True\n",
              "23  0.714453    0.670853    0.643762  ...      0.4  0.0010        True\n",
              "27  0.705724    0.643046    0.649283  ...      0.5  0.0010       False\n",
              "25  0.674164    0.643345    0.623337  ...      0.5  0.0001       False\n",
              "20  0.658957    0.717995    0.608367  ...      0.3  0.0001        True\n",
              "22  0.675346    0.682264    0.621037  ...      0.4  0.0001        True\n",
              "12  0.647551    0.753725    0.587832  ...      0.2  0.0001        True\n",
              "21  0.697662    0.703045    0.628461  ...      0.3  0.0010        True\n",
              "14  0.687237    0.743360    0.617762  ...      0.2  0.0010        True\n",
              "15  0.659742    0.746001    0.601636  ...      0.2  0.0010       False\n",
              "13  0.635557    0.757164    0.586625  ...      0.2  0.0001       False\n",
              "19  0.679728    0.740170    0.619116  ...      0.2  0.0010        True\n",
              "18  0.646724    0.755071    0.590270  ...      0.2  0.0001        True\n",
              "16  0.827478    0.726217    0.650141  ...      0.2  0.0100        True\n",
              "6   0.631303    0.806648    0.574442  ...      0.1  0.0001        True\n",
              "11  0.761266    0.770519    0.618038  ...      0.1  0.0100       False\n",
              "7   0.617374    0.825036    0.570751  ...      0.1  0.0001       False\n",
              "10  0.817157    0.765236    0.642579  ...      0.1  0.0100        True\n",
              "8   0.672446    0.788409    0.606914  ...      0.1  0.0010        True\n",
              "4   0.793923    0.847062    0.629595  ...      0.0  0.0100        True\n",
              "17  0.774960    0.729107    0.625415  ...      0.2  0.0100       False\n",
              "9   0.639962    0.800169    0.586478  ...      0.1  0.0010       False\n",
              "3   0.631846    0.922161    0.581065  ...      0.0  0.0010       False\n",
              "0   0.601899    0.937011    0.560864  ...      0.0  0.0001        True\n",
              "5   0.735450    0.866746    0.610516  ...      0.0  0.0100       False\n",
              "2   0.644315    0.902626    0.592149  ...      0.0  0.0010        True\n",
              "1   0.597124    0.962376    0.703883  ...      0.0  0.0001       False\n",
              "\n",
              "[28 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFO1VBqU7k5Z"
      },
      "source": [
        "**Training Model with Finalized Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6BIoJTsie8l"
      },
      "source": [
        "# GRU Optimal Hyperparameters\n",
        "def RNN_Model():\n",
        "    \n",
        "    text_sequence = Input(shape = (max_length,), name = 'text_sequence_input')\n",
        "    rnn_layer = Embedding(num_words, EMBEDDING_DIM, weights = [embedding_matrix], trainable = False, name = 'embedding')(text_sequence)\n",
        "    rnn_layer = Dropout(0.5)(rnn_layer)\n",
        "    rnn_layer = GRU(units = 32, dropout = 0.5,  recurrent_regularizer = l2(0.001))(rnn_layer)\n",
        "    rnn_layer = BatchNormalization()(rnn_layer)\n",
        "    rnn_layer = Dense(32, activation = 'relu', name = 'dense', kernel_regularizer = l2(0.001))(rnn_layer)\n",
        "    output = Dense(1, activation = 'sigmoid', name = 'output')(rnn_layer) # Change to 1 if it's just classification\n",
        "    \n",
        "    model = Model(inputs = text_sequence, outputs = output)\n",
        "    return model"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4KNwFnsie-s",
        "outputId": "704939af-c023-411c-d4f8-527d0f4e7436"
      },
      "source": [
        "model = RNN_Model()\n",
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_sequence_input (InputLa [(None, 16)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 16, 100)           1445100   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 100)           0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 32)                12864     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,459,181\n",
            "Trainable params: 14,017\n",
            "Non-trainable params: 1,445,164\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuUPAE3RiDFg",
        "outputId": "c5bdeed8-5fc8-40dd-a2de-b7092b02d20d"
      },
      "source": [
        "# Early stopping and model checkpoint\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'drive/MyDrive/CIS520 Project/final dl models/word2vec_gru_title1.h5', monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "\n",
        "# Train the DL Model\n",
        "model.compile(loss = keras.losses.BinaryCrossentropy(from_logits = True), optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size = 32, epochs = 20, validation_data = (X_val, y_val), verbose = 1,\n",
        "         callbacks = [early_stopping])\n",
        "\n",
        "results = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Unfreeze embedding layer\n",
        "model.layers[1].trainable = True\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.0005), metrics = ['accuracy'])\n",
        "history2 = model.fit(X_train, y_train, batch_size = 32, epochs = 30, validation_data = (X_val, y_val), verbose = 1,\n",
        "         callbacks = [early_stopping, model_checkpoint])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "628/628 [==============================] - 4s 6ms/step - loss: 0.7140 - accuracy: 0.5489 - val_loss: 0.6754 - val_accuracy: 0.6143\n",
            "Epoch 2/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6830 - accuracy: 0.5841 - val_loss: 0.6629 - val_accuracy: 0.6488\n",
            "Epoch 3/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6756 - accuracy: 0.6039 - val_loss: 0.6577 - val_accuracy: 0.6526\n",
            "Epoch 4/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6718 - accuracy: 0.6084 - val_loss: 0.6532 - val_accuracy: 0.6637\n",
            "Epoch 5/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6698 - accuracy: 0.6145 - val_loss: 0.6526 - val_accuracy: 0.6614\n",
            "Epoch 6/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6692 - accuracy: 0.6133 - val_loss: 0.6506 - val_accuracy: 0.6749\n",
            "Epoch 7/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6660 - accuracy: 0.6239 - val_loss: 0.6524 - val_accuracy: 0.6795\n",
            "Epoch 8/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6666 - accuracy: 0.6227 - val_loss: 0.6488 - val_accuracy: 0.6747\n",
            "Epoch 9/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6654 - accuracy: 0.6268 - val_loss: 0.6492 - val_accuracy: 0.6610\n",
            "Epoch 10/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6651 - accuracy: 0.6251 - val_loss: 0.6468 - val_accuracy: 0.6869\n",
            "Epoch 11/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6645 - accuracy: 0.6287 - val_loss: 0.6463 - val_accuracy: 0.6881\n",
            "Epoch 12/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6641 - accuracy: 0.6281 - val_loss: 0.6485 - val_accuracy: 0.6618\n",
            "Epoch 13/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6627 - accuracy: 0.6303 - val_loss: 0.6479 - val_accuracy: 0.6602\n",
            "Epoch 14/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6622 - accuracy: 0.6329 - val_loss: 0.6445 - val_accuracy: 0.6897\n",
            "Epoch 15/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6626 - accuracy: 0.6362 - val_loss: 0.6453 - val_accuracy: 0.6849\n",
            "Epoch 16/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6624 - accuracy: 0.6285 - val_loss: 0.6446 - val_accuracy: 0.6835\n",
            "Epoch 17/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6617 - accuracy: 0.6365 - val_loss: 0.6445 - val_accuracy: 0.6831\n",
            "Epoch 18/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6625 - accuracy: 0.6337 - val_loss: 0.6443 - val_accuracy: 0.6801\n",
            "Epoch 19/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6610 - accuracy: 0.6379 - val_loss: 0.6439 - val_accuracy: 0.6877\n",
            "Epoch 20/20\n",
            "628/628 [==============================] - 3s 5ms/step - loss: 0.6605 - accuracy: 0.6370 - val_loss: 0.6433 - val_accuracy: 0.6863\n",
            "195/195 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6558\n",
            "Epoch 1/30\n",
            "628/628 [==============================] - 12s 19ms/step - loss: 0.7093 - accuracy: 0.6577 - val_loss: 0.5736 - val_accuracy: 0.7148\n",
            "Epoch 2/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.6091 - accuracy: 0.6710 - val_loss: 0.5657 - val_accuracy: 0.7213\n",
            "Epoch 3/30\n",
            "628/628 [==============================] - 11s 18ms/step - loss: 0.6020 - accuracy: 0.6795 - val_loss: 0.5582 - val_accuracy: 0.7283\n",
            "Epoch 4/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.5915 - accuracy: 0.6864 - val_loss: 0.5503 - val_accuracy: 0.7279\n",
            "Epoch 5/30\n",
            "628/628 [==============================] - 11s 17ms/step - loss: 0.5821 - accuracy: 0.6974 - val_loss: 0.5394 - val_accuracy: 0.7369\n",
            "Epoch 6/30\n",
            "628/628 [==============================] - 11s 17ms/step - loss: 0.5681 - accuracy: 0.7118 - val_loss: 0.5297 - val_accuracy: 0.7453\n",
            "Epoch 7/30\n",
            "628/628 [==============================] - 10s 17ms/step - loss: 0.5568 - accuracy: 0.7177 - val_loss: 0.5225 - val_accuracy: 0.7506\n",
            "Epoch 8/30\n",
            "628/628 [==============================] - 11s 17ms/step - loss: 0.5390 - accuracy: 0.7355 - val_loss: 0.5092 - val_accuracy: 0.7566\n",
            "Epoch 9/30\n",
            "628/628 [==============================] - 11s 17ms/step - loss: 0.5280 - accuracy: 0.7412 - val_loss: 0.4949 - val_accuracy: 0.7678\n",
            "Epoch 10/30\n",
            "628/628 [==============================] - 11s 17ms/step - loss: 0.5141 - accuracy: 0.7495 - val_loss: 0.4887 - val_accuracy: 0.7724\n",
            "Epoch 11/30\n",
            "628/628 [==============================] - 10s 17ms/step - loss: 0.4977 - accuracy: 0.7638 - val_loss: 0.4823 - val_accuracy: 0.7782\n",
            "Epoch 12/30\n",
            "628/628 [==============================] - 11s 18ms/step - loss: 0.4814 - accuracy: 0.7736 - val_loss: 0.4727 - val_accuracy: 0.7863\n",
            "Epoch 13/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.4711 - accuracy: 0.7793 - val_loss: 0.4773 - val_accuracy: 0.7913\n",
            "Epoch 14/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.4594 - accuracy: 0.7852 - val_loss: 0.4581 - val_accuracy: 0.7941\n",
            "Epoch 15/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.4451 - accuracy: 0.7957 - val_loss: 0.4485 - val_accuracy: 0.8019\n",
            "Epoch 16/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.4302 - accuracy: 0.8042 - val_loss: 0.4408 - val_accuracy: 0.8025\n",
            "Epoch 17/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.4268 - accuracy: 0.8086 - val_loss: 0.4299 - val_accuracy: 0.8077\n",
            "Epoch 18/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.4133 - accuracy: 0.8170 - val_loss: 0.4285 - val_accuracy: 0.8116\n",
            "Epoch 19/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3991 - accuracy: 0.8249 - val_loss: 0.4271 - val_accuracy: 0.8120\n",
            "Epoch 20/30\n",
            "628/628 [==============================] - 11s 17ms/step - loss: 0.3926 - accuracy: 0.8273 - val_loss: 0.4307 - val_accuracy: 0.8164\n",
            "Epoch 21/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3820 - accuracy: 0.8337 - val_loss: 0.4168 - val_accuracy: 0.8186\n",
            "Epoch 22/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3769 - accuracy: 0.8340 - val_loss: 0.4142 - val_accuracy: 0.8230\n",
            "Epoch 23/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3652 - accuracy: 0.8425 - val_loss: 0.4148 - val_accuracy: 0.8242\n",
            "Epoch 24/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3568 - accuracy: 0.8453 - val_loss: 0.4107 - val_accuracy: 0.8234\n",
            "Epoch 25/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3481 - accuracy: 0.8481 - val_loss: 0.4112 - val_accuracy: 0.8266\n",
            "Epoch 26/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3441 - accuracy: 0.8529 - val_loss: 0.4108 - val_accuracy: 0.8316\n",
            "Epoch 27/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3401 - accuracy: 0.8526 - val_loss: 0.4080 - val_accuracy: 0.8294\n",
            "Epoch 28/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3281 - accuracy: 0.8600 - val_loss: 0.4066 - val_accuracy: 0.8330\n",
            "Epoch 29/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3271 - accuracy: 0.8629 - val_loss: 0.4138 - val_accuracy: 0.8312\n",
            "Epoch 30/30\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.3162 - accuracy: 0.8665 - val_loss: 0.4056 - val_accuracy: 0.8346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDJWE3DnCK5B",
        "outputId": "c6a3db63-c270-4bf8-e918-eb78625d1dfd"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.0005), metrics = ['accuracy'])\r\n",
        "history2 = model.fit(X_train, y_train, batch_size = 32, epochs = 5, validation_data = (X_val, y_val), verbose = 1,\r\n",
        "         callbacks = [early_stopping, model_checkpoint])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "628/628 [==============================] - 10s 17ms/step - loss: 0.2919 - accuracy: 0.8805 - val_loss: 0.4000 - val_accuracy: 0.8364\n",
            "Epoch 2/5\n",
            "628/628 [==============================] - 10s 17ms/step - loss: 0.2904 - accuracy: 0.8800 - val_loss: 0.3911 - val_accuracy: 0.8401\n",
            "Epoch 3/5\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.2801 - accuracy: 0.8841 - val_loss: 0.3969 - val_accuracy: 0.8445\n",
            "Epoch 4/5\n",
            "628/628 [==============================] - 10s 16ms/step - loss: 0.2802 - accuracy: 0.8820 - val_loss: 0.3963 - val_accuracy: 0.8419\n",
            "Epoch 5/5\n",
            "628/628 [==============================] - 10s 15ms/step - loss: 0.2757 - accuracy: 0.8882 - val_loss: 0.3942 - val_accuracy: 0.8431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2djxslswCpw",
        "outputId": "58f76c6f-7f5b-4880-e02c-5b625f403df0"
      },
      "source": [
        "# Save model\n",
        "model.save('drive/MyDrive/CIS520 Project/final dl models/word2vec_final')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: drive/MyDrive/CIS520 Project/final dl models/word2vec_final/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUxJUs7C7_BC"
      },
      "source": [
        "**Load Previously Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH2mRgDfraz-",
        "outputId": "c7bb0b19-689a-4a3c-a82e-296b2e04451f"
      },
      "source": [
        "# Evaluate on test set\r\n",
        "results = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195/195 [==============================] - 0s 2ms/step - loss: 0.8681 - accuracy: 0.7027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmmYOgbbra2B"
      },
      "source": [
        "y_test_probs = model.predict(X_test)\r\n",
        "y_test_preds = (y_test_probs > 0.5).astype(int)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56cRBjD4xRkG"
      },
      "source": [
        "def get_classification_metrics(actual, pred):\r\n",
        "  print(confusion_matrix(actual, pred))\r\n",
        "  print('Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}'.format(\r\n",
        "      accuracy_score(actual, pred),\r\n",
        "      precision_score(actual, pred),\r\n",
        "      recall_score(actual, pred),\r\n",
        "      f1_score(actual, pred)))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ7mikLvra4W",
        "outputId": "a2bcee86-9c4b-450b-d6d3-4467dca235c2"
      },
      "source": [
        "get_classification_metrics(y_test, y_test_preds)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2391  729]\n",
            " [1262 1858]]\n",
            "Accuracy: 0.6809294871794872, Precision: 0.718206416698879, Recall: 0.5955128205128205, F1 Score: 0.6511301909935168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0UIy1ofzqfM"
      },
      "source": [
        "# Get the predictions for test_final\r\n",
        "\r\n",
        "X_test_final = test_final_vectorized\r\n",
        "y_test_final = test_final['top25pct'].to_numpy()\r\n",
        "y_test_final_probs = model.predict(X_test_final).ravel()\r\n",
        "test_final_out = pd.DataFrame(data = {'id': test_final['id'], 'prediction': y_test_final_probs})\r\n",
        "\r\n",
        "test_final_out.to_csv('drive/MyDrive/CIS520 Project/deeplearning_preds.csv', index = None)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypw2oWssv6Ws"
      },
      "source": [
        "**Model Interpretability using ELI5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRH9hi1dyhUf"
      },
      "source": [
        "import eli5\n",
        "from eli5.lime import TextExplainer"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APNHBXZ6tEyy"
      },
      "source": [
        "# Define the custom predict function - input is list of strings (documents) and return matrix of shape (n_samples, n_classes) with probability values\n",
        "\n",
        "# Assumes you already fitted the tokenizer on the training data\n",
        "def predict_complex(documents_list):\n",
        "\n",
        "  # Generate the sequence of tokens\n",
        "  # tokenizer (from above)\n",
        "  sequences = tokenizer.texts_to_sequences(documents_list)\n",
        "\n",
        "  # Pad the sequences\n",
        "  X = pad_sequences(sequences, maxlen = 16)\n",
        "\n",
        "  # Predict\n",
        "  y_high_probs = model.predict(X, batch_size = 32, verbose = 0)\n",
        "  y_low_probs = (1 - y_high_probs)\n",
        "  y_combined_probs = np.hstack((y_low_probs, y_high_probs))  \n",
        "  \n",
        "  return y_combined_probs"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "DYojM6wPtE1D",
        "outputId": "0c596ddc-6096-43ae-cb24-f715295801a6"
      },
      "source": [
        "te = TextExplainer(random_state = 42)\n",
        "\n",
        "num_titles = 5\n",
        "indexes = random.choices(train.index, k = num_titles)\n",
        "\n",
        "doc = ' '.join(train['processed_title'].iloc[indexes[0]])\n",
        "\n",
        "# Create your own article\n",
        "doc = 'daily penn analytics code machine learning forecast website readers'\n",
        "te.fit(doc, predict_complex)\n",
        "te.explain_prediction(target_names = ['low', 'high'])"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=high\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.988</b>, score <b>4.390</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +4.280\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 98.46%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.110\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(120, 100.00%, 91.75%); opacity: 0.82\" title=\"0.183\">daily</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.19%); opacity: 0.86\" title=\"0.550\">penn</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.90%); opacity: 0.85\" title=\"-0.394\">analytics</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.45%); opacity: 0.82\" title=\"-0.161\">code</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 66.57%); opacity: 0.95\" title=\"1.351\">machine</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.01%); opacity: 0.83\" title=\"-0.276\">learning</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"1.746\">forecast</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.89%); opacity: 0.82\" title=\"0.211\">website</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 70.16%); opacity: 0.93\" title=\"1.149\">readers</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "Explanation(estimator=\"SGDClassifier(alpha=0.001, average=False, class_weight=None,\\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\\n              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\\n              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\\n              power_t=0.5, random_state=RandomState(MT19937) at 0x7F60678E1150,\\n              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\\n              warm_start=False)\", description=None, error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target='high', feature_weights=FeatureWeights(pos=[FeatureWeight(feature='machine', weight=1.3492686465465347, std=None, value=1.0), FeatureWeight(feature='forecast', weight=1.3467841088549464, std=None, value=1.0), FeatureWeight(feature='readers', weight=1.1489545813759445, std=None, value=1.0), FeatureWeight(feature='penn', weight=0.6586349748544948, std=None, value=1.0), FeatureWeight(feature='learning forecast', weight=0.25063746169137907, std=None, value=1.0), FeatureWeight(feature='daily', weight=0.1830023229890028, std=None, value=1.0), FeatureWeight(feature='code', weight=0.15298464579187912, std=None, value=1.0), FeatureWeight(feature='forecast website', weight=0.14863832786343126, std=None, value=1.0), FeatureWeight(feature='<BIAS>', weight=0.11001379941407897, std=None, value=1.0), FeatureWeight(feature='machine learning', weight=0.10267847496953476, std=None, value=1.0), FeatureWeight(feature='website', weight=0.06239973543784683, std=None, value=1.0)], neg=[FeatureWeight(feature='learning', weight=-0.6289583482451098, std=None, value=1.0), FeatureWeight(feature='analytics code', weight=-0.21319467262761174, std=None, value=1.0), FeatureWeight(feature='penn analytics', weight=-0.10903335879988428, std=None, value=1.0), FeatureWeight(feature='code machine', weight=-0.10094988001191169, std=None, value=1.0), FeatureWeight(feature='analytics', weight=-0.07154964034806517, std=None, value=1.0)], pos_remaining=0, neg_remaining=0), proba=0.9877549295929116, score=4.390311179756491, weighted_spans=WeightedSpans(docs_weighted_spans=[DocWeightedSpans(document='daily penn analytics code machine learning forecast website readers', spans=[('daily', [(0, 5)], 0.1830023229890028), ('penn', [(6, 10)], 0.6586349748544948), ('analytics', [(11, 20)], -0.07154964034806517), ('code', [(21, 25)], 0.15298464579187912), ('machine', [(26, 33)], 1.3492686465465347), ('learning', [(34, 42)], -0.6289583482451098), ('forecast', [(43, 51)], 1.3467841088549464), ('website', [(52, 59)], 0.06239973543784683), ('readers', [(60, 67)], 1.1489545813759445), ('penn analytics', [(6, 10), (11, 20)], -0.10903335879988428), ('analytics code', [(11, 20), (21, 25)], -0.21319467262761174), ('code machine', [(21, 25), (26, 33)], -0.10094988001191169), ('machine learning', [(26, 33), (34, 42)], 0.10267847496953476), ('learning forecast', [(34, 42), (43, 51)], 0.25063746169137907), ('forecast website', [(43, 51), (52, 59)], 0.14863832786343126)], preserve_density=False, vec_name=None)], other=FeatureWeights(pos=[FeatureWeight(feature=<FormattedFeatureName 'Highlighted in text (sum)'>, weight=4.280297380342412, std=None, value=None), FeatureWeight(feature='<BIAS>', weight=0.11001379941407897, std=None, value=1.0)], neg=[], pos_remaining=0, neg_remaining=0)), heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "z-_nPc0_8LND",
        "outputId": "b480161a-4535-48eb-ff7d-cfca2a610522"
      },
      "source": [
        "te.explain_weights(target_names = ['low', 'high'])"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "            \n",
              "                \n",
              "                \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=high\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.349\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        machine\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.03%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.347\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        forecast\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 82.13%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.149\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        readers\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.89%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.659\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        penn\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.84%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.251\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        learning forecast\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 95.06%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.183\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        daily\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 95.64%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.153\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        code\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 95.73%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.149\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        forecast website\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 96.54%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.110\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 96.70%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.103\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        machine learning\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 97.67%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.062\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        website\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 97.44%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.072\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        analytics\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 96.74%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.101\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        code machine\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 96.56%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.109\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        penn analytics\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 94.50%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.213\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        analytics code\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.28%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.629\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        learning\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "            \n",
              "        \n",
              "\n",
              "        \n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "Explanation(estimator=\"SGDClassifier(alpha=0.001, average=False, class_weight=None,\\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\\n              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\\n              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\\n              power_t=0.5, random_state=RandomState(MT19937) at 0x7F60678E1150,\\n              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\\n              warm_start=False)\", description=\"\\nFeatures with largest coefficients.\\nCaveats:\\n1. Be careful with features which are not\\n   independent - weights don't show their importance.\\n2. If scale of input features is different then scale of coefficients\\n   will also be different, making direct comparison between coefficient values\\n   incorrect.\\n3. Depending on regularization, rare features sometimes may have high\\n   coefficients; this doesn't mean they contribute much to the\\n   classification result for most examples.\\n\", error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target='high', feature_weights=FeatureWeights(pos=[FeatureWeight(feature='machine', weight=1.3492686465465347, std=None, value=None), FeatureWeight(feature='forecast', weight=1.3467841088549464, std=None, value=None), FeatureWeight(feature='readers', weight=1.1489545813759445, std=None, value=None), FeatureWeight(feature='penn', weight=0.6586349748544948, std=None, value=None), FeatureWeight(feature='learning forecast', weight=0.25063746169137907, std=None, value=None), FeatureWeight(feature='daily', weight=0.1830023229890028, std=None, value=None), FeatureWeight(feature='code', weight=0.15298464579187912, std=None, value=None), FeatureWeight(feature='forecast website', weight=0.14863832786343126, std=None, value=None), FeatureWeight(feature='<BIAS>', weight=0.11001379941407897, std=None, value=None), FeatureWeight(feature='machine learning', weight=0.10267847496953476, std=None, value=None), FeatureWeight(feature='website', weight=0.06239973543784683, std=None, value=None)], neg=[FeatureWeight(feature='learning', weight=-0.6289583482451098, std=None, value=None), FeatureWeight(feature='analytics code', weight=-0.21319467262761174, std=None, value=None), FeatureWeight(feature='penn analytics', weight=-0.10903335879988428, std=None, value=None), FeatureWeight(feature='code machine', weight=-0.10094988001191169, std=None, value=None), FeatureWeight(feature='analytics', weight=-0.07154964034806517, std=None, value=None)], pos_remaining=0, neg_remaining=0), proba=None, score=None, weighted_spans=None, heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    }
  ]
}