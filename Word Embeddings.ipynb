{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and Deep Learning Article Views\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:47:54.958331Z",
     "start_time": "2020-12-02T07:47:47.928212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\isaac\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:41:19.930986Z",
     "start_time": "2020-12-02T07:41:15.854593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:47:56.083993Z",
     "start_time": "2020-12-02T07:47:54.960326Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:47:56.099494Z",
     "start_time": "2020-12-02T07:47:56.085495Z"
    }
   },
   "outputs": [],
   "source": [
    "content = train['content'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:47:56.115480Z",
     "start_time": "2020-12-02T07:47:56.102451Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(content_list):\n",
    "    \n",
    "    processed_list = []\n",
    "    \n",
    "    for line in tqdm(content_list):\n",
    "        tokens = word_tokenize(line)\n",
    "        # Convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # Remove punctuation\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # Remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # Filter out stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        \n",
    "        processed_list.append(words)\n",
    "        \n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T04:38:09.723625Z",
     "start_time": "2020-12-02T04:38:09.705559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing - to remove length-1 words, and remove non-alphabet symbols\n",
    "def preprocessing(titles_array):\n",
    "\n",
    "    processed_array = []\n",
    "    \n",
    "    for title in tqdm(titles_array):\n",
    "        \n",
    "        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n",
    "        processed = re.sub('[^a-zA-Z ]', '', title)\n",
    "        \n",
    "        words = processed.split()\n",
    "        \n",
    "        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n",
    "        processed_array.append(' '.join([word for word in words if len(word) > 1]))\n",
    "    \n",
    "    return processed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:49:17.102840Z",
     "start_time": "2020-12-02T07:47:57.184639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 16772/16772 [01:19<00:00, 210.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the words\n",
    "train['processed_content'] = preprocessing(train['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec Model\n",
    "\n",
    "Word2Vec is a static word-embedding.\n",
    "\n",
    "**Getting from Word Embedding to Doc Embedding**\n",
    "\n",
    "- https://towardsdatascience.com/nlp-performance-of-different-word-embeddings-on-text-classification-de648c6262b\n",
    "- 3 methods:\n",
    "1. Simple Averaging of Word Embedding\n",
    "2. TF-IDF Weighted Averaging on Word Embedding\n",
    "3. Directly leverage Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T10:42:24.842171Z",
     "start_time": "2020-12-02T10:42:24.827216Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T10:43:10.271415Z",
     "start_time": "2020-12-02T10:42:26.608752Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = train['processed_content'], size = EMBEDDING_DIM, min_count = 1, window = 5, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T10:43:10.303405Z",
     "start_time": "2020-12-02T10:43:10.280399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 111788\n"
     ]
    }
   ],
   "source": [
    "# Vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T10:43:52.301926Z",
     "start_time": "2020-12-02T10:43:52.278818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('erode', 0.7844998836517334),\n",
       " ('defy', 0.7674049139022827),\n",
       " ('distorting', 0.7603688836097717),\n",
       " ('arrogance', 0.7515872120857239),\n",
       " ('induce', 0.7422409653663635),\n",
       " ('useless', 0.7407965660095215),\n",
       " ('indulge', 0.7380375862121582),\n",
       " ('conceive', 0.7355968356132507),\n",
       " ('unchecked', 0.7346436381340027),\n",
       " ('virtue', 0.7324776649475098)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model\n",
    "model.wv.most_similar('bear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T10:44:32.065565Z",
     "start_time": "2020-12-02T10:44:32.019971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughter', 0.8678452372550964),\n",
       " ('son', 0.8435013890266418),\n",
       " ('remembered', 0.8427660465240479),\n",
       " ('sheldon', 0.8286240100860596),\n",
       " ('haymes', 0.8157896399497986),\n",
       " ('hackney', 0.811972439289093),\n",
       " ('stormin', 0.8090441226959229),\n",
       " ('husband', 0.807654082775116),\n",
       " ('wife', 0.805396556854248),\n",
       " ('dad', 0.8004140853881836)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing some math on the word vectors\n",
    "model.wv.most_similar_cosmul(positive = ['father', 'female'], negative = ['male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:23:54.242999Z",
     "start_time": "2020-12-02T06:23:45.103128Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "filename = 'word2vec_train2.txt'\n",
    "model.wv.save_word2vec_format(filename, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2Vec Model for Training Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:49:19.674207Z",
     "start_time": "2020-12-02T07:49:17.104806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the embeddings from the stored file\n",
    "# Embedding is size 111k (# words) x 100 (dimensions)\n",
    "import os \n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec_train2.txt'), encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:49:24.330489Z",
     "start_time": "2020-12-02T07:49:19.676204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize the text samples into 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "# Fit the tokenizer on the text\n",
    "tokenizer_obj.fit_on_texts(train['processed_content'])\n",
    "# Generate the sequence of tokens\n",
    "sequences = tokenizer_obj.texts_to_sequences(train['processed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:49:24.346222Z",
     "start_time": "2020-12-02T07:49:24.332611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the max length of each article - 5587\n",
    "max_length = max([len(s) for s in train['processed_content']])\n",
    "# Get vocab size\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sequences is len = 16772 (# articles)\n",
    "- review_pad has shape (16772, 5587)\n",
    "- 111788 unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:49:24.822841Z",
     "start_time": "2020-12-02T07:49:24.348131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "review_pad = pad_sequences(sequences, maxlen = max_length)\n",
    "\n",
    "word_index = tokenizer_obj.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:50:00.565704Z",
     "start_time": "2020-12-02T07:49:55.580455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "num_words = len(word_index) + 1\n",
    "words_not_found = []\n",
    "# Create the emedding matrix - map embeddings from word2vec model for each word and create matrix of word vectors\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words: # Least common words (don't care)\n",
    "        continue\n",
    "        \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if (embedding_vector is not None):\n",
    "        # Assign the ith elmenet of the embedding matrix to the embedding of that word\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "        \n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:50:03.665840Z",
     "start_time": "2020-12-02T07:50:03.654317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111789, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
